"""
Market Data Processing Performance Tests
æµ‹è¯•è¡Œæƒ…æ•°æ®å¤„ç†æ€§èƒ½
"""
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from datetime import datetime
from perpbot.events.event_bus import EventBus
from perpbot.events.event_types import MarketDataUpdate

from benchmark_config import get_benchmark, TEST_CONFIG
from benchmark_utils import BenchmarkRunner, PerformanceReporter


def test_market_data_update():
    """æµ‹è¯•è¡Œæƒ…æ•°æ®æ›´æ–°å¤„ç†æ€§èƒ½"""
    # åˆå§‹åŒ–
    event_bus = EventBus()
    received_count = 0

    def on_market_data(event: MarketDataUpdate):
        nonlocal received_count
        received_count += 1

    event_bus.subscribe(MarketDataUpdate, on_market_data)

    # å¾…æµ‹è¯•å‡½æ•°
    def publish_market_data():
        event = MarketDataUpdate(
            exchange="okx",
            symbol="BTC-USDT",
            bid=50000.0,
            ask=50001.0,
            bid_size=1.5,
            ask_size=2.0,
            timestamp=datetime.utcnow(),
        )
        event_bus.publish(event)

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = get_benchmark("market_data_update")
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"],
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=TEST_CONFIG["memory_profiling"],
    )

    metrics = runner.run_benchmark(
        name="Market Data Update Processing",
        func=publish_market_data,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics, benchmark)

    # éªŒè¯
    assert received_count == TEST_CONFIG["test_iterations"], \
        f"Expected {TEST_CONFIG['test_iterations']} events, got {received_count}"

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert metrics.latency_mean <= benchmark.max_latency_ms, \
        f"Mean latency {metrics.latency_mean:.3f}ms exceeds max {benchmark.max_latency_ms}ms"
    assert metrics.throughput >= benchmark.min_throughput, \
        f"Throughput {metrics.throughput:.2f} ops/s below min {benchmark.min_throughput} ops/s"

    print("âœ… Market Data Update test PASSED")

    return metrics


def test_event_dispatch():
    """æµ‹è¯• EventBus äº‹ä»¶åˆ†å‘æ€§èƒ½"""
    event_bus = EventBus()
    received = []

    # æ³¨å†Œå¤šä¸ªè®¢é˜…è€…
    def subscriber1(event: MarketDataUpdate):
        received.append(1)

    def subscriber2(event: MarketDataUpdate):
        received.append(2)

    def subscriber3(event: MarketDataUpdate):
        received.append(3)

    event_bus.subscribe(MarketDataUpdate, subscriber1)
    event_bus.subscribe(MarketDataUpdate, subscriber2)
    event_bus.subscribe(MarketDataUpdate, subscriber3)

    # å¾…æµ‹è¯•å‡½æ•°
    def dispatch_event():
        event = MarketDataUpdate(
            exchange="okx",
            symbol="ETH-USDT",
            bid=3000.0,
            ask=3001.0,
            bid_size=10.0,
            ask_size=12.0,
            timestamp=datetime.utcnow(),
        )
        event_bus.publish(event)
        received.clear()  # æ¸…ç†ï¼Œé¿å…å†…å­˜ç´¯ç§¯

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = get_benchmark("event_dispatch")
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"],
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=TEST_CONFIG["memory_profiling"],
    )

    metrics = runner.run_benchmark(
        name="EventBus Event Dispatch",
        func=dispatch_event,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics, benchmark)

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert metrics.latency_mean <= benchmark.max_latency_ms, \
        f"Mean latency {metrics.latency_mean:.3f}ms exceeds max {benchmark.max_latency_ms}ms"
    assert metrics.throughput >= benchmark.min_throughput, \
        f"Throughput {metrics.throughput:.2f} ops/s below min {benchmark.min_throughput} ops/s"

    print("âœ… EventBus Dispatch test PASSED")

    return metrics


if __name__ == "__main__":
    print("=" * 80)
    print("Market Data Processing Performance Tests")
    print("=" * 80)
    print()

    all_metrics = []

    # è¿è¡Œæµ‹è¯•
    try:
        print("\nğŸš€ Test 1: Market Data Update Processing")
        m1 = test_market_data_update()
        all_metrics.append(m1)

        print("\nğŸš€ Test 2: EventBus Event Dispatch")
        m2 = test_event_dispatch()
        all_metrics.append(m2)

    except AssertionError as e:
        print(f"\nâŒ Test FAILED: {e}")
        sys.exit(1)

    # ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(TEST_CONFIG["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)

    report_file = output_dir / f"market_data_perf_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    PerformanceReporter.generate_markdown_report(all_metrics, str(report_file))

    print("\nâœ… All tests PASSED!")
    print(f"ğŸ“Š Report: {report_file}")
