"""
Arbitrage Scanner Performance Tests
æµ‹è¯•å¥—åˆ©æ‰«æå™¨æ€§èƒ½
"""
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from datetime import datetime
from perpbot.scanner.market_scanner_v3 import MarketScannerV3
from perpbot.scanner.scanner_config import ScannerConfig
from perpbot.events.event_bus import EventBus

from benchmark_config import get_benchmark, TEST_CONFIG
from benchmark_utils import BenchmarkRunner, PerformanceReporter


def create_mock_scanner() -> MarketScannerV3:
    """åˆ›å»ºæ¨¡æ‹Ÿæ‰«æå™¨"""
    config = ScannerConfig(
        enabled_exchanges=["okx", "paradex", "hyperliquid"],
        enabled_symbols=["BTC-USDT", "ETH-USDT"],
        min_profit_bps=10,
        update_interval_sec=1.0,
        max_position_size_usd=1000.0,
    )

    event_bus = EventBus()
    scanner = MarketScannerV3(config=config, event_bus=event_bus)

    return scanner


def test_arbitrage_scan():
    """æµ‹è¯•å¥—åˆ©æœºä¼šæ‰«ææ€§èƒ½"""
    scanner = create_mock_scanner()

    # æ¨¡æ‹Ÿè¡Œæƒ…æ•°æ®
    mock_quotes = {
        "okx": {
            "BTC-USDT": {"bid": 50000.0, "ask": 50010.0, "bid_size": 1.0, "ask_size": 1.0},
            "ETH-USDT": {"bid": 3000.0, "ask": 3005.0, "bid_size": 5.0, "ask_size": 5.0},
        },
        "paradex": {
            "BTC-USDT": {"bid": 50020.0, "ask": 50030.0, "bid_size": 0.8, "ask_size": 0.8},
            "ETH-USDT": {"bid": 3010.0, "ask": 3015.0, "bid_size": 4.0, "ask_size": 4.0},
        },
        "hyperliquid": {
            "BTC-USDT": {"bid": 50005.0, "ask": 50015.0, "bid_size": 1.2, "ask_size": 1.2},
            "ETH-USDT": {"bid": 3002.0, "ask": 3008.0, "bid_size": 6.0, "ask_size": 6.0},
        },
    }

    # å¾…æµ‹è¯•å‡½æ•°
    def scan_opportunities():
        # æ¨¡æ‹Ÿæ‰«æé€»è¾‘ï¼šéå†äº¤æ˜“å¯¹ï¼Œè®¡ç®—ä»·å·®
        opportunities = []
        symbols = ["BTC-USDT", "ETH-USDT"]

        for symbol in symbols:
            # æ‰¾åˆ°æœ€é«˜ bid å’Œæœ€ä½ ask
            best_bid = max(
                (ex, mock_quotes[ex][symbol]["bid"])
                for ex in mock_quotes
                if symbol in mock_quotes[ex]
            )
            best_ask = min(
                (ex, mock_quotes[ex][symbol]["ask"])
                for ex in mock_quotes
                if symbol in mock_quotes[ex]
            )

            bid_exchange, bid_price = best_bid
            ask_exchange, ask_price = best_ask

            # è®¡ç®—ä»·å·®ï¼ˆBPSï¼‰
            if bid_price > ask_price:
                spread_bps = ((bid_price - ask_price) / ask_price) * 10000
                if spread_bps >= scanner.config.min_profit_bps:
                    opportunities.append({
                        "symbol": symbol,
                        "buy_exchange": ask_exchange,
                        "sell_exchange": bid_exchange,
                        "spread_bps": spread_bps,
                        "buy_price": ask_price,
                        "sell_price": bid_price,
                    })

        return opportunities

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = get_benchmark("arbitrage_scan")
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"],
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=TEST_CONFIG["memory_profiling"],
    )

    metrics = runner.run_benchmark(
        name="Arbitrage Opportunity Scan",
        func=scan_opportunities,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics, benchmark)

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert metrics.latency_mean <= benchmark.max_latency_ms, \
        f"Mean latency {metrics.latency_mean:.3f}ms exceeds max {benchmark.max_latency_ms}ms"
    assert metrics.throughput >= benchmark.min_throughput, \
        f"Throughput {metrics.throughput:.2f} ops/s below min {benchmark.min_throughput} ops/s"

    print("âœ… Arbitrage Scan test PASSED")

    return metrics


def test_spread_calculation():
    """æµ‹è¯•ä»·å·®è®¡ç®—æ€§èƒ½"""
    # å¾…æµ‹è¯•å‡½æ•°
    def calculate_spread():
        buy_price = 50000.0
        sell_price = 50100.0
        spread_bps = ((sell_price - buy_price) / buy_price) * 10000
        return spread_bps

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"] * 10,  # æ›´å¤šè¿­ä»£ï¼Œå› ä¸ºè¿™æ˜¯è½»é‡æ“ä½œ
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=False,  # ä¸éœ€è¦å†…å­˜åˆ†æ
    )

    metrics = runner.run_benchmark(
        name="Spread Calculation",
        func=calculate_spread,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics)

    print("âœ… Spread Calculation test PASSED")

    return metrics


if __name__ == "__main__":
    print("=" * 80)
    print("Arbitrage Scanner Performance Tests")
    print("=" * 80)
    print()

    all_metrics = []

    # è¿è¡Œæµ‹è¯•
    try:
        print("\nğŸš€ Test 1: Arbitrage Opportunity Scan")
        m1 = test_arbitrage_scan()
        all_metrics.append(m1)

        print("\nğŸš€ Test 2: Spread Calculation")
        m2 = test_spread_calculation()
        all_metrics.append(m2)

    except AssertionError as e:
        print(f"\nâŒ Test FAILED: {e}")
        sys.exit(1)

    # ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(TEST_CONFIG["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)

    report_file = output_dir / f"arbitrage_scanner_perf_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    PerformanceReporter.generate_markdown_report(all_metrics, str(report_file))

    print("\nâœ… All tests PASSED!")
    print(f"ğŸ“Š Report: {report_file}")
