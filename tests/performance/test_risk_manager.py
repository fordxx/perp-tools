"""
Risk Manager Performance Tests
æµ‹è¯•é£æ§ç®¡ç†å™¨æ€§èƒ½
"""
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from datetime import datetime
from perpbot.risk_manager import RiskManager, RiskCheckResult
from perpbot.events.event_bus import EventBus

from benchmark_config import get_benchmark, TEST_CONFIG
from benchmark_utils import BenchmarkRunner, PerformanceReporter


def create_mock_risk_manager() -> RiskManager:
    """åˆ›å»ºæ¨¡æ‹Ÿé£æ§ç®¡ç†å™¨"""
    event_bus = EventBus()

    risk_manager = RiskManager(
        max_position_size=10000.0,
        max_daily_loss=500.0,
        max_drawdown_percent=5.0,
        event_bus=event_bus,
    )

    return risk_manager


def test_risk_check():
    """æµ‹è¯•é£æ§æ£€æŸ¥æ€§èƒ½"""
    risk_manager = create_mock_risk_manager()

    # æ¨¡æ‹Ÿäº¤æ˜“è¯·æ±‚
    trade_request = {
        "symbol": "BTC-USDT",
        "size": 0.1,
        "side": "long",
        "exchange": "okx",
        "price": 50000.0,
        "notional": 5000.0,
    }

    # å¾…æµ‹è¯•å‡½æ•°
    def perform_risk_check():
        # ç®€åŒ–çš„é£æ§æ£€æŸ¥é€»è¾‘
        checks = []

        # æ£€æŸ¥1: ä»“ä½å¤§å°
        position_ok = trade_request["notional"] <= risk_manager.max_position_size
        checks.append(("position_size", position_ok))

        # æ£€æŸ¥2: æ€»é£é™©æ•å£
        total_exposure = 15000.0  # æ¨¡æ‹Ÿå½“å‰æ€»æ•å£
        exposure_ok = total_exposure + trade_request["notional"] <= risk_manager.max_position_size * 3
        checks.append(("total_exposure", exposure_ok))

        # æ£€æŸ¥3: æ—¥å†…äºæŸ
        daily_pnl = -100.0  # æ¨¡æ‹Ÿæ—¥å†…äºæŸ
        loss_ok = abs(daily_pnl) <= risk_manager.max_daily_loss
        checks.append(("daily_loss", loss_ok))

        # æ±‡æ€»ç»“æœ
        all_passed = all(check[1] for check in checks)

        return {
            "passed": all_passed,
            "checks": checks,
            "timestamp": datetime.utcnow(),
        }

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = get_benchmark("risk_check")
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"],
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=TEST_CONFIG["memory_profiling"],
    )

    metrics = runner.run_benchmark(
        name="Risk Manager Check",
        func=perform_risk_check,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics, benchmark)

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert metrics.latency_mean <= benchmark.max_latency_ms, \
        f"Mean latency {metrics.latency_mean:.3f}ms exceeds max {benchmark.max_latency_ms}ms"
    assert metrics.throughput >= benchmark.min_throughput, \
        f"Throughput {metrics.throughput:.2f} ops/s below min {benchmark.min_throughput} ops/s"

    print("âœ… Risk Check test PASSED")

    return metrics


def test_exposure_calculation():
    """æµ‹è¯•é£é™©æ•å£è®¡ç®—æ€§èƒ½"""
    # æ¨¡æ‹ŸæŒä»“æ•°æ®
    positions = [
        {"symbol": "BTC-USDT", "size": 0.5, "entry_price": 50000.0, "side": "long"},
        {"symbol": "ETH-USDT", "size": 2.0, "entry_price": 3000.0, "side": "long"},
        {"symbol": "BTC-USDT", "size": -0.3, "entry_price": 50100.0, "side": "short"},
    ]

    # å¾…æµ‹è¯•å‡½æ•°
    def calculate_exposure():
        total_long = 0.0
        total_short = 0.0

        for pos in positions:
            notional = abs(pos["size"]) * pos["entry_price"]
            if pos["side"] == "long":
                total_long += notional
            else:
                total_short += notional

        net_exposure = total_long - total_short

        return {
            "total_long": total_long,
            "total_short": total_short,
            "net_exposure": net_exposure,
        }

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = get_benchmark("exposure_calculation")
    runner = BenchmarkRunner(
        warmup_iterations=TEST_CONFIG["warmup_iterations"],
        test_iterations=TEST_CONFIG["test_iterations"],
        percentiles=TEST_CONFIG["percentiles"],
        enable_memory_profiling=TEST_CONFIG["memory_profiling"],
    )

    metrics = runner.run_benchmark(
        name="Exposure Calculation",
        func=calculate_exposure,
    )

    # æ‰“å°ç»“æœ
    PerformanceReporter.print_metrics(metrics, benchmark)

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert metrics.latency_mean <= benchmark.max_latency_ms, \
        f"Mean latency {metrics.latency_mean:.3f}ms exceeds max {benchmark.max_latency_ms}ms"
    assert metrics.throughput >= benchmark.min_throughput, \
        f"Throughput {metrics.throughput:.2f} ops/s below min {benchmark.min_throughput} ops/s"

    print("âœ… Exposure Calculation test PASSED")

    return metrics


if __name__ == "__main__":
    print("=" * 80)
    print("Risk Manager Performance Tests")
    print("=" * 80)
    print()

    all_metrics = []

    # è¿è¡Œæµ‹è¯•
    try:
        print("\nğŸš€ Test 1: Risk Manager Check")
        m1 = test_risk_check()
        all_metrics.append(m1)

        print("\nğŸš€ Test 2: Exposure Calculation")
        m2 = test_exposure_calculation()
        all_metrics.append(m2)

    except AssertionError as e:
        print(f"\nâŒ Test FAILED: {e}")
        sys.exit(1)

    # ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(TEST_CONFIG["output_dir"])
    output_dir.mkdir(parents=True, exist_ok=True)

    report_file = output_dir / f"risk_manager_perf_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    PerformanceReporter.generate_markdown_report(all_metrics, str(report_file))

    print("\nâœ… All tests PASSED!")
    print(f"ğŸ“Š Report: {report_file}")
